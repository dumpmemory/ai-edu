
## 9.3 策略对价值函数的影响

### 9.3.1 给定策略下的状态函数值

让我们先暂时从穿越虫洞的问题中回到射击气球的问题，因为还有一个疑问要解决。

前面也提到过，这个策略是通过统计游客的行为而得到的。意味着在100个游客中，有40个游客会选择射击红色气球，另外60个游客会选择射击蓝色气球，两轮射击都是如此。

所以，这还是没有回到我们在第 8 章最开始提出的问题：你做为一个聪明的游客你该如何选择呢？因为不可能把一支箭分成 0.4:0.6，所以你必须二选一，同时记住你的目的是达到最大收益。

$$
\pi(a \mid s)=
\begin{cases}
0.4, & a=射击红球
\\
0.6, & a=射击蓝球
\end{cases}
$$


一个不怎么“聪明”的游客也会想到应该连续两次选择射击红球才有可能得到 6 分的奖励，在这里所谓“聪明”与否完全是对风险的判断以及自身实力的评估，从而选择连续两次射击蓝球而不是红球，但这不是强化学习的范畴。

所以，让我们回到强化学习领域来，看看最佳的策略应该是什么。

<center>
<img src="./img/shoot-result.png">

图 9.3.1
</center>



在马尔可夫决策过程过程的概念中学习过，所谓策略就是在每个状态上采取什么动作，从而使收益最大化。在图 9.3.1 中，一共有 6 个有效的状态（$s_0,\cdots,s_5$) ，在每个状态上的策略有两种动作选择（红球或蓝球），这样一共有 $2^6$ 种选择。

初始化环境是指定的策略是这样的：
```python
    Policy = {          # 策略
        0:[0.4,0.6],    # 在状态 0 时，选择射击红球的概率0.4，选择射击蓝球的概率0.6
        1:[0.4,0.6],    # 在状态 1 时，同上
        2:[0.4,0.6],
        3:[0.4,0.6],
        4:[0.4,0.6],
        5:[0.4,0.6],
        6:[0.4,0.6]     # 可以不定义，因为在终止状态没有动作
    }
```

但是 0.4:0.6 这个数字，只是游乐场老板统计的结果，对于你来说这是最好的策略吗？

### 9.3.2 尝试不同的策略带来的影响

为了验证该策略对最终结果的影响，我们可以尝试修改一下该策略，然后比较“开始(0)”状态的价值函数的值，看看谁高谁低。

【代码位置】Shoot_0_Try_Policy.py

```python
import numpy as np
import copy
import Shoot_2_DataModel as dataModel
import Algo_PolicyValueFunction as algo

if __name__=="__main__":
    Policy = {      # 原始状态
        ...         # 同上
    }
    gamma = 1
    max_iteration = 1000
    env = dataModel.Env(Policy)
    V,Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)
    print("在原始策略下的状态价值函数值")
    print(np.round(V,5))
    print("在原始策略下的动作价值函数值")
    print(Q)
    ......
```

这一部分的输出

```
在原始策略下的状态价值函数值
[1.19548 0.56    0.554   0.8     0.56    0.73    0.     ]
在原始策略下的动作价值函数值
[[1.0957 1.262 ]
 [0.5    0.6   ]
 [0.56   0.55  ]
 [0.8    0.8   ]
 [0.5    0.6   ]
 [0.7    0.75  ]
 [0.     0.    ]]
```

修改策略，看影响

```python
    ......(接上一段代码)
    # 新策略
    test_policy = np.array([
        [0.2,0.8],  # 修改状态 0 的策略
        [0.5,0.5],  # 修改状态 1 的策略
        [0.3,0.7],  # 修改状态 2 的策略
        [0.1,0.9],  # 修改状态 3 的策略
        [0.3,0.7],  # 修改状态 4 的策略
        [0.6,0.4]   # 修改状态 5 的策略
    ])
    # 每次只修改一个策略,保持其它策略不变,以便观察其影响
    for i in range(6):
        print(str.format("修改状态 {0} 的策略:{1}", i, test_policy[i]))
        new_policy = copy.deepcopy(Policy)  # 继承原始策略
        new_policy[i] = test_policy[i]      # 只修改其中一个状态的策略
        env = dataModel.Env(new_policy)
        V,Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)
        print(np.round(V,5))
```       
输出

```
修改状态 0 的策略:[0.2 0.8]
[1.22874 0.56    0.554   0.8     0.56    0.73    0.     ]
修改状态 1 的策略:[0.5 0.5]
[1.19228 0.55    0.554   0.8     0.56    0.73    0.     ]
修改状态 2 的策略:[0.3 0.7]
[1.19546 0.56    0.553   0.8     0.56    0.73    0.     ]
修改状态 3 的策略:[0.1 0.9]
[1.19548 0.56    0.554   0.8     0.56    0.73    0.     ]
修改状态 4 的策略:[0.3 0.7]
[1.19788 0.56    0.554   0.8     0.57    0.73    0.     ]
修改状态 5 的策略:[0.6 0.4]
[1.19188 0.56    0.554   0.8     0.56    0.72    0.     ]
```

把结果列在表中

表 9.3.1

|策略 $\to$ 价值|$v_0$|$v_1$|$v_2$|$v_3$|$v_4$|$v_5$|$v_6$|
|-|-|-|-|-|-|-|-|
|原始策略的基准值:[0.4, 0.6] |1.19548 | 0.56 | 0.554 | 0.8 | 0.56 | 0.73 | 0 |
|修改 $s_0$ 的策略 $\pi'_0$:[0.2, 0.8]|**1.22874**| 0.56|    0.554| 0.8| 0.56 | 0.73 | 0|
|修改 $s_1$ 的策略 $\pi'_1$:[0.7, 0.3]|**1.19228**|**0.55**| 0.554| 0.8| 0.56| 0.73 |0|
|修改 $s_2$ 的策略 $\pi'_2$:[0.3, 0.7]|**1.19546**| 0.56 | **0.553**| 0.8| 0.56| 0.73 | 0|
|修改 $s_3$ 的策略 $\pi'_3$:[0.1, 0.9]|1.19548| 0.56| 0.554| 0.8 |0.56 | 0.73 | 0|
|修改 $s_4$ 的策略 $\pi'_4$:[0.5, 0.5]|**1.19788**|0.56| 0.554|0.8|**0.57**|0.73|0|
|修改 $s_5$ 的策略 $\pi'_5$:[0.6, 0.4]|**1.19188**| 0.56|0.554 |0.8 | 0.56 |**0.72** | 0|

第一行为原始策略的基准值

后面每行中，我们把与基准值相比有变化的值都用黑体标了出来，便于读者可以快速发现区别。

把状态(0) 的策略 $\pi_0$ 从 [0.4,0.6] 修改为 [0.2,0.8] 后，对后续状态的价值函数都没有影响，但是 $v_0$ 的价值函数提高到了 1.22874（基准值为 1.19548）。

原因

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s)q_\pi(s,a)
\tag{由8.4.5}
$$

蓝(1) 的动作价值函数比红(0) 高，原始策略蓝(1) 占比 0.6，新策略蓝(0) 占比 0.8，所以最后会影响到开始(0)的价值，

$$v_\pi(s_0) = \sum_{a \in (a_0,a_1)} \pi_0(a|s_0)q_\pi(s_0,a)=0.4\times1.0957+0.6\times1.262 = 1.19548
\\
v'_\pi(s_0) = \sum_{a \in (a_0,a_1)} \pi'_0(a|s_0)q_\pi(s_0,a)= 0.2 \times 1.0957 + 0.8 \times 1.262 = 1.22874
$$

注意，$\pi_0$ 的修改只会影响到状态 $s_0$

$\pi_1$ 的修改会影响到状态值 $v_\pi(s_1)$，进而会影响到它的上游的 $q_\pi(s_0,a_0)$ 的值，以及再上游的状态 $v_\pi(s_0)$ 的值。


为什么对状态 $s_3$ 的策略 $\pi_3$ 修改没有影响到任何状态值的变化呢？

因为它的下游动作价值函数 $q_\pi(s_3,a_0)=q_\pi(s_3,a_1)=0.8$，所以 $0.4\times 0.8 + 0.6 \times 0.8=0.1 \times 0.8 + 0.9 \times 0.8=0.8$


 $\pi'_0, \pi'_4$ 的修改会提高 $v_\pi(s_0)$ 的值