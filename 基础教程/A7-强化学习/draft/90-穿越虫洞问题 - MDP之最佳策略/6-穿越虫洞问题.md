
因为在射击问题中，有以下几个特点：

- 是一个单向选择和转移的过程，最后形成了一张有向无环图。这就决定了我们可以从后向前一步步**回溯**，自下向上逐层计算状态价值函数 $v_\pi(s)$ 和动作价值函数 $q_\pi(s,a)$，直到开始状态。其计算过程是：$v_T \to q_\pi \to v_\pi \to q_\pi \to v_\pi$，如图 mdp-8 所示。

- 每个分支的细节分析起来比较多，但实际上分支之间的区别只存在于状态转移的过程的具体概率数值上。比如图 mdp-7 所示，其中的“红3”动作后的状态转移概率是 [0.25,0.7.0.05]，而动作“红5”的状态转移概率是 [0.2,0.75,0.05]。这种细节只是导致计算结果不同，但概念是相同的。

- 而另外一个重要的策略选择问题，我们暂时用“红:蓝=0.4:0.6”的统计结果设定了，在两次射击时都是如此。但是在实际问题中，游客各有各的策略，这种统一的设定只是一种统计结果。游乐场老板虽然精明，但是游客们也不是傻子，谁都知道连续两次选择红色气球是有机会得到最高值 6 分的奖励的，只不过是能不能顺利实现的问题。


### 迭代问题

在前面的射击气球问题中，我们根据贝尔曼期望方程中的状态价值函数 $v_\pi$ 和动作价值函数 $q_\pi$ 的定义，用反推的方法，手工计算出了各个节点的价值函数，以加深对价值函数定义的理解。

但是，如果有些问题没有定义终止状态的话，我们该从何下手来计算呢？

其实在学习马尔可夫奖励过程和贝尔曼方程时，曾经使用迭代方法来得到状态价值函数 $v$ 的收敛值，而在这里可以使用同样的方法来解决没有终止状态的贝尔曼期望方程的问题。


### 穿越虫洞问题

<center>
<img src="./img/ship-1.png">

图 1
</center>

问题描述

- 在一个 5x5 的宇宙空间中，一艘探索太空的宇宙飞船可以任意向四个方向行驶，策略 $\pi=0.25$。

- 比如在 $s_{18}$ 处，如果选择向右行驶，将会以 $p=1.0$ 的转移概率达到 19，并得到 0 的奖励。

- 如果在如 $s_{24}$ 所示的角落处向右行驶或向下行驶，将会碰撞能量屏障使飞船受损，飞船位置不发生改变，得到 -1 的“奖励”，但并非终止状态。其它角落处也是如此，一共有 4 个角落状态（序号为：0，4，20，24）。

- 如果在如 $s_9$ 所示的边界处向右行驶，将会碰撞能量屏障使飞船受损，得到 -1 的“奖励”，飞船位置不发生改变，但并非终止状态，还可以进一步行驶。其它边界处也是如此，一共有 12 个边界状态（序号为：1，2，3，5，9，10，14，15，19，21，22，23）。

- 在 $s_1$ 和 $s_3$ 处有两个虫洞：

    - 在 $s_1$ 处进行下一步行驶时，无论任何方向，将无条件地达到 $s_{12}$ 处，并得到 +5 的奖励，但后者并非终止状态。

    - 在 $s_3$ 处进行下一步行驶时，无论任何方向，将无条件地达到 $s_{21}$ 处，并得到 +10 的奖励，但后者并非终止状态。

    所以序号为 1,3 的两个状态可以从边界状态中去掉。

注意几点：

1. 没有终止状态，也就是说没有分幕，飞船可以一直行驶。
2. 到达 $s_1,s_3$ 时，不是被立刻吸入虫洞，而是要进行下一步动作时才会时空转移。也就说在 $s_1,s_3$ 并没有机会向上行驶而出界。
3. 关于边角位置的状态，如图 x 所示，以状态 $s_2$ 为例：
    - 如果从该状态以 0.25 的概率选择向上移动，会以概率 1.0 回到 $s_2$，并有 -1 的奖励。所以说，这里面既有策略 $\pi$，又有转移概率 $p$，只不过只有一个下游状态，没有分支。
    - 如果从 $s_2$ 以 0.25 的概率选择向下移动，会以 1.0 的概率转移到 $s_7$，得到 0 的奖励。
<center>
<img src="./img/ship-2.png">

图 6
</center>



没有终点状态的话，我们无法确定任意一个状态的价值函数，进而算出其上游的动作价值函数。所以，我们必须研究一下马尔可夫决策过程下的贝尔曼期望方程的迭代解法了。

### 二级回溯

从前面的推导中，可以在已知 $q_\pi(s,a)$ 时计算出 $v_\pi(s)$（式5），也可以已知 $v_\pi(s)$ 计算出 $q_\pi(s,a)$（式2），这似乎变成了一个鸡生蛋蛋生鸡的死循环问题。能不能像贝尔曼方程那样，从$v_\pi(s')$ 计算出 $v_\pi(s)$ 来，从$q_\pi(s',a')$ 计算出 $q_\pi(s,a)$ 来呢？这样的话，我们就可以继续利用前面学过的迭代法或者矩阵法来方便地解出状态价值函数和动作价值函数了。

先绘制出两张二级回溯图，方便公式推导。如图 6 所示。

<center>
<img src="./img/mdp-7.png">

图 6
</center>

先看左图，我们的目的是想从 $v_\pi(s')$ 得到 $v_\pi(s)$，中间隔着一个 $q_\pi(s,a)$。如果把 $q_\pi(s,a)$ 的表达式带入 $v_\pi(s)$ 的表达式，就可以达到消去 $q_\pi(s,a)$ 的目的了。


于是可以把式 2.1 的 $q_\pi(s,a)$ 带入式 5：


$$
\begin{aligned}
v_\pi(s) &= \sum_a \pi(a|s)q_\pi(s,a) &(式5)
\\
(带入式2.1替换 q_\pi \to)&=\sum_a \pi(a|s) \Big(\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')]\Big) &(10.1)
\\
(矩阵形式\to)&=\sum_a \pi(a|s) \Big(P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')]\Big) &(10.2)
\\
(动作奖励函数形式\to)&=\sum_a \pi(a|s) \Big( R^a(s)+ \gamma P_{ss'}^a V_\pi(s') \Big) &(10.3)
\end{aligned}
\tag{10}
$$

式 10 就是 $v_\pi$ 的迭代表达式。

再看右图，想把$v_\pi(s')$ 消掉，需要先得到它的表达式。

从式 5 把 $s,a$ 换成 $s',a'$，可以得到式 11：

$$
v_\pi(s') = \sum_{a'} \pi(a'|s')q_\pi(s',a')
\tag{11}
$$

把式 11 带入式 2：

$$
\begin{aligned}
q_\pi(s,a)&=\sum_{s'} p_{ss'}^a \big [r_{ss'}^a+\gamma v_\pi(s') \big ] &(式2.1)
\\
(带入式11替换v_\pi\to)&=\sum_{s'} p_{ss'}^a \Big ( r_{ss'}^a+\gamma \sum_{a'} [\pi(a'|s')q_\pi(s',a') ] \Big) &(12.1)
\\
(动作奖励函数形式 \to)&=R^a(s)+\gamma \sum_{s'} p_{ss'}^a \Big ( \sum_{a'} [\pi(a'|s')q_\pi(s',a')] \Big) &(12.2)
\end{aligned}
\tag{12}
$$

式 12 就是 $q_\pi$ 的迭代的表达形式。

本节的公式有些多，下面总结一下，便于读者以后速查。

表 3 $v_\pi, q_\pi$ 的互换速查表

|=|$v_\pi$|$q_\pi$|
|-|-|-|
|$v_\pi$|$v_\pi(s)=\sum_a \pi(a \mid s) \Big(\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')]\Big)$<br>(式10.1)|$v_\pi(s) = \sum_{a} \pi(a \mid s)q_\pi(s,a)$ <br>(式5)|
|$q_\pi$|$q_\pi(s,a)=\sum_{s'} p_{ss'}^a \big [r_{ss'}^a+\gamma v_\pi(s') \big ]$<br> (式2.1)|$q_\pi(s,a)=\sum_{s'} p_{ss'}^a \Big ( r_{ss'}^a+\gamma \sum_{a'} [\pi(a'\mid s')q_\pi(s',a') ] \Big)$<br> (式12.1)|

在表 3 中，我们只给出了公式的原始形式，读者可以在实际用使用矩阵形式或是奖励函数形式。
