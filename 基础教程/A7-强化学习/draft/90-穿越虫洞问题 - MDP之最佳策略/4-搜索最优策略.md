
## 9.4 搜索最优策略

上一小节中，我们搞清楚了策略对价值的影响根据具体数值不同而有正有负，所以，现在的任务就是给每个状态都找到最好的策略，从而使整体状态值达到最佳。

但是在具体操作中会遇到这样的困难：假设我们知道了一个状态上的最佳策略是 [0.4,0.6]，但是不可能把一支箭分成 0.4:0.6，所以你必须二选一，用公式表示为：

$$
\pi(a \mid s)=
\begin{cases}
1, & if \ a=\argmax A(s)
\\
0, & 其它动作
\end{cases}
\tag{9.4.1}
$$

当 $\pi=[0.4,0.6], A(s)=[a_0,a_1]$ 时，式 9.4.1 的结果是：$\pi(a_0|s)=0,\pi(a_1|s)=1$。因为 0.6 > 0.4 且处于数组中第 1 个位置（从 0 开始计算）。


所以，让我们回到强化学习领域来，看看最佳的策略应该是什么。



在马尔可夫决策过程过程的概念中学习过，所谓策略就是在每个状态上采取什么动作，从而使收益最大化。在图 9.3.1 中，一共有 6 个有效的状态（$s_0,\cdots,s_5$) ，在每个状态上的策略有两种动作选择（红球或蓝球），这样一共有 $2^6$ 种选择。


###  策略搜索

我们对 $\pi$ 的修改会提高或降低 $v_\pi(s_0)$ 的值，这还只是独立修改一个策略的影响，那么一共有多少种组合可以帮助游客提高状态价值呢？其最大值又是多少呢？

推广到一般情况：如果状态空间为 $S$，动作空间为 $A$，则策略组合是 $A^S$ 种。

对于这个简单的问题来说，即使用遍历的方法，也很容易快速得到结果。所以，我们先用最笨但是最准确的遍历来得到评估的基准。


用搜索法
遍历6个状态下（s0-s5）的所有策略选择




而图 x 所示的最佳选择是第一轮射击蓝色气球，第二轮继续射击蓝色气球。

但是，图 x 的各个节点的价值函数，包括状态价值函数和动作价值函数，是在一个固定的策略下计算出来的，即：




