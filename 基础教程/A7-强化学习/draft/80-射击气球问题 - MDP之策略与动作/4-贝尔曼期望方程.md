## 8.4 贝尔曼期望方程

有了模型和奖励，就可以进一步地研究价值问题了。如同马尔可夫奖励过程中的状态价值函数一样，在马尔可夫奖励过程过程中，同样会有价值函数，而且概念进一步地深化和扩展了。


### 8.4.1 奖励过程和决策过程的比较

我们首先用图 8.4.1 来比较一下两种过程，避免概念混淆。

<center>
<img src="./img/mdp-3.png">

图 8.4.1 左侧：马尔可夫奖励过程模型（贝尔曼方程）；右侧：马尔可夫决策过程模型（贝尔曼期望方程）。
</center>

表 8.4.1 比较图 8.4.1 中的左右两部分

||左侧|右侧|
|-|-|-|
|名称|马尔科夫奖励过程 MRP|马尔可夫决策过程 MDP
|模型|两层节点，一层过程|三层节点，两层过程|
|上层节点|源状态|源状态|
|中层节点|无|动作节点|
|下层节点|目标状态|目标状态|
|过程|实线箭头为状态转移 $P_{ss'}$ 以及过程奖励 $R_{ss'}$|实线箭头为策略选择$\pi(a \mid s)$，虚线箭头为状态转移$P^a_{ss'}$以及过程奖励$R^a_{ss'}$|
|解法|贝尔曼方程|贝尔曼期望方程|
|状态价值函数定义| $ v(s)= \mathbb E [G_t \mid S_t=s]$ | $v_\pi(s,a)=\mathbb E[G_t \mid S_t=s]$|
|动作价值函数定义|无|$q_\pi(s,a)=\mathbb E[G_t \mid S_t=s,A_t=a]$|

$v_\pi(s)$ 比 $v(s)$ 多了一个下标 $\pi$，是为了区分二者，没有实际的数学含义。

在有的资料中，给贝尔曼期望方程的 $\mathbb E$ 写作 $\mathbb E_\pi$，也是这个意思，没有实际的数学含义。


表 8.4.2 虚线框内的部分的局部比较

||左侧|右侧|
|-|-|-|
|顶端节点|源状态 $s_0$，需要计算状态价值函数$v(s_0)$|源动作 $a_1$，需要计算动作价值函数$q_{\pi}(s_0,a_1)$|
|中间过程|状态转移概率$P_{ss'}$，过程奖励向量$R_{ss'}$|状态转移概率$P_{ss'}^{a_1}$，过程奖励向量$R_{ss'}^{a_1}$|
|底端节点|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v(s')$|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v_{\pi}(s')$|

比较图 8.4.1 中左侧和右侧虚线框内的部分，可以说除了符号不同，其它都是相同的，包括位置和含义。

### 8.4.2 动作价值函数 $q_\pi$

先回忆一下在马尔可夫奖励过程中学习过的状态价值函数，温故而知新。

$$
\begin{aligned}
v(s) &= \mathbb E [G_t \mid S_t = s]
\\
&=\mathbb E [R_{t+1}\mid S_t=s] + \gamma \mathbb E[G_{t+1}\mid S_t=s]
\\
&=\sum_{s'} p_{ss'} r_{ss'}+ \gamma \sum_{s'} p_{ss'}v(s') =\sum_{s'} p_{ss'} [r_{ss'}+\gamma v(s')] 
\\
&= P_{ss'} R_{ss'} + \gamma P_{ss'} V(s')
\\
&= R(s)+ \gamma P_{ss'}V(s') 
\end{aligned}
\tag{8.4.1}
$$

观察贝尔曼方程的推导，其本质是：某个状态的价值函数 $v(s)$ 由三部分组成：
1. 其下游状态的价值 $v(s')$；
2. 转移概率 $p$；
2. 转移过程中的奖励 $r$。

无巧不成书，在下面推导动作价值函数 $q_\pi$ 的公式时，我们也遇到了和式 8.4.1 同样的表达。所以，我们可以大胆地预测，计算动作价值函数 $q_\pi(s,a)$ 的公式与计算状态价值函数 $v(s)$ 的贝尔曼方程完全一致。

$$
\begin{aligned}
q_\pi(s,a) &= \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=\mathbb E [R_{t+1}\mid S_t=s, A_t=a] + \gamma \mathbb E[G_{t+1}\mid S_t=s, A_t=a]
\\
&=\sum_{s'} p_{ss'}^a r_{ss'}^a+ \gamma \sum_{s'} p_{ss'}^a v_\pi(s') =\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')] &(1)
\\
&=P^a_{ss'} R^a_{ss'} + \gamma P^a_{ss'} V_\pi(s')=P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')] &(2)
\\
&= R^a(s)+ \gamma P_{ss'}^a V_\pi(s')  &(3)
\end{aligned}
\tag{8.4.2}
$$


比较式 8.4.1 和式 8.4.2，对于本章的动作价值函数来说，除了在条件部分多出来一个 $A_t=a$ 以外，其它的部分完全相同，所以它们的表达式也应该相同，但是含义不同，前者是简单的状态价值函数，后者是有策略下的动作价值函数。

那么多出来的这个 $A_t=a$ 会造成什么不同吗？答案是不会。因为这个条件相当于在图 8.4.1 中右侧的部分确定了是选择 $a_1$ 还是 $a_2$，正是因为有了这个条件存在，才会让黄色虚线框部分的模型结构高度相似。


### 8.4.3 状态价值函数 $v_\pi$

函数名称定义为 $v_\pi$ 的原因是为了和马尔可夫过程的状态价值函数 $v$ 区分开来，其定义是：

$$
v_\pi(s) = \mathbb E [G_t \mid S_t=s] \tag{8.4.3}
$$

同前面一样，式 8.4.3 仍然是要求回报 $G_t$ 的数学期望。在图 8.4.1 的右侧，我们考虑以 $v_\pi(s_0)$ 为例推出通用的价值函数公式。

状态 $s_0$ 并不直接接触到奖励机制，而是通过策略 $\pi$ 与下游的两个动作 $a_1,a_2$ 连接，所以，一旦知道了 $a_1,a_2$ 的动作价值函数 $q_\pi$，那么 $s_0$ 的状态价值函数就可以表示为 $q_\pi$ 的期望了，即：

$$
\begin{aligned}
v_\pi(s_0) &=\pi_1 q_\pi(s_0,a_1) + \pi_2 q_\pi(s_0,a_2)
\\
&=\sum_{a \in A(s)} \pi(a \mid s_0) q_\pi(s_0,a)
\end{aligned}
\tag{8.4.4}
$$

所以，式 8.4.3 可以引申为：

$$
v_\pi(s) = \mathbb E [G_t \mid S_t=s] = \sum_{a \in A(s)} \pi(a|s)q_\pi(s,a)
\tag{8.4.5}
$$
