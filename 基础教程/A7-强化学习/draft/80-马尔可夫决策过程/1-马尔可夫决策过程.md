## 马尔可夫决策过程

### 射击气球问题

我们仍然以前面学过的、熟悉的状态转移问题来开始本章的学习。

在游乐场，有一个射击中奖的游戏规则是这样的：

1. 游客花 4 元钱买 2 颗橡皮子弹，得到两次射击机会；
2. 墙上有两个气球，一个蓝色大气球，一个红色小气球，游客可以任意选择目标；
3. 击中蓝色大气球可以得到小奖，价值 1 元；
4. 击中红色小气球可以得到大奖，价值 3 元。

游乐场老板的经过几个月的营业后进行了统计，结果如图 1 所示。

<center>
<img src="./img/shoot-1.png">

图 1 游客中奖情况统计
</center>

1. 游客在第一次射击时：

    - 中大奖的概率是 0.08
    - 脱靶的概率是 0.54
    - 中小奖的概率是 0.38


2. 第二次射击根据第一次射击的结果而有所不同：

    比如，如果第一次射击中大奖，则：
    - 第二次射击继续中大奖的概率是 0.10，比初始的 0.08 提高了一些，因为游客有经验了；
    - 第二次射击脱靶的概率是 0.49，比初始的 0.54 要低；
    - 第二次射击中小奖的概率是 0.41，比第一次的 0.38 要高，也是因为有了一定的经验而提高了命中率。
    
    其它统计数字在图 1 中显示，不再赘述。

那么一个聪明的游客该如何选择呢？

利用前面学到过的知识，可以计算第一次射击时的三个状态，哪一个的状态价值函数值最大。于是有些读者前面的学习基础很牢靠，可以立刻画出如图 2 的状态转移图和转移概率表。

<center>
<img src="./img/shoot-2.png">

图 2 错误的状态转移图
</center>

其实这个问题的状态转移图应该如图 3 所示，是一个有向无环图。

<center>
<img src="./img/shoot-3.png">

图 3 正确的的状态转移图
</center>

说明

1. 虽然状态中有四个“大奖”，四个“小奖”，四个“脱靶”，但是这四个状态不是同一个状态，只是名字一样而已，在图 3 中特意用序号把它们都区分开了。其中：
    - 0 号为开始状态；
    - 1~3 号为二级状态；
    - 4~12 号为三级状态。
2. 开始状态需要有个 -4 的奖励，因为一开始游客是花 4 元钱买了两颗橡皮子弹。
3. 所有的三级状态（序号4~12），最后都接一个终止状态 T。

为什么四个“大奖”不是一个状态呢？

从“开始”状态看，第一次射击有 0.1 的概率到达 1 号“大奖”状态；而第二次射击有 0.2 的概率到达 4 号“大奖”状态。如果游客还有第三发子弹，那么中大奖的概率在第三轮还会提高。就好比从一楼上 10 级台阶到二楼，台阶可以定义为 1 到 10；而二楼到三楼同样有 10 级台阶，也定义为 1 到 10，但所处的楼层不一样。

我们从后向前计算一下各个状态的价值函数，由于是个有向无环的图，所以根据式 xxx 很简单就可以得到（本例中折扣应该为 1）：

首先，终止状态 $v_T = 0$。

然后 $v_4$ 为例：$v_4 = R_4 + \gamma 1.0 \times V_T = 3+1\times0=3$。其它三级状态的计算方法相同。

$$
\begin{cases}
v_T=0
\\
v_4=3, v_5 = 0, v_6 = 1, 
\\
v_7=3, v_8=0, v_9=1,
\\
v_{10}=3, v_{11}=0, v_{12}=1,
\\
v_1 = 3 + 0.2v_4+0.2v_5+0.6v_6=4.2
\\
v_2 = 0 + 0.1v_7+0.3v_8+0.6v_9=0.9
\\
v_3 = 1 + 0.1v_{10}+0.2v_{11}+0.7v_{12}=1.8
\\
v_0 = -4 + 0.1v_1+0.3v_2+0.6v_3=-2.23
\end{cases}
$$

所以，从统计学的观点看，只要游客买了子弹，就已经亏了，老板是稳赚不赔的。

那么游客应该怎么选择呢？

- 虽然 $V_{1}=4.2$ 状态价值最高，但是到达 $s_1$ 只有 0.1 的概率，可以理解为游客的收益是：$0.1\times4.2=0.42$元。
- $s_2$ 的收益是 $0.3\times0.9=0.27$ 元。
- $s_3$ 的收益是 $0.6\times1.8=1.08$ 元。

所以游客第一次射击时应该选择 $s_3$，即“小奖”状态，然后第二次射击时继续选择“小奖”状态。这样虽然肯定不能赢回开始花的 4 元钱，但是可以做到损失最小（或者收益最大）。


### 改进 引入动作/策略

仔细想一想，其实上面的解题过程是有问题的：

1. 原题是“聪明的游客会如何选择？”，但是整个解题思路是根据游乐场老板的统计结果进行的，是从老板的角度出发，而非游客的角度。
2. 在一级的三个状态中（大奖，小奖，脱靶），它只代表“结果”，而不代表“选择”，较真儿地说的话，没有游客会选择“脱靶”状态，因为“脱靶”只是个结果。
3. 忽略前面两个疑问，最后得到了 $v_1,v_2,v_3$ 的值，游客会看到 $v_1$ 最大，所以会误导其选择大奖而射击小气球，但是忽略了击中的难度。现实生活中人们往往也有类似的经历，想利益最大化，却忽略了风险。

实际上，老板并不知道游客选择的是哪个气球，他只看到了最终的结果。游客自己也不会主动说我要射击哪一个气球，只不过心里有数罢了。

经过对游客的调查与过程观察，我们得到了一些数据，如图 4 所示。

<center>
<img src="./img/shoot-4.png">

图 4 正确的的状态转移图
</center>


1. 在开始状态时，游客的策略（称之为 $\pi$）可以有两个：
    - $a_1$ - 选择射击小气球而中大奖，大概有 40% 的人选择。
    - $a_2$ - 选择射击大气球而中小奖，大概有 60% 的人选择。

2. 选择 $a_1$（记为 $\pi(a_1|s)$），在射击小气球时：
    - 只有 0.25 的概率打中，因为气球比较小，不容易击中，记为 $p_{11}=0.25$；
    - 却有 0.7 的概率脱靶，记为 $p_{12}=0.7$；
    - 还有 0.05 的概率很离谱地击中大气球而中小奖，记为 $p_{13}=0.05$。

3. 选择 $a_2$（记为 $\pi(a_2|s)$），在射击大气球时：
    - 有 0.8 的概率击中，因为气球比较大，容易击中，记为 $p_{23}=0.8$；
    - 有 0.2 的概率脱靶，记为 $p_{22}=0.2$；
    - 没有任何运气可以击中小气球，记为 $p_{21}=0.0$。
        - 这种情况在实际解题时可以不必画出来，在这里画出来是为了把细节讲清楚，避免误解。

现在可以统计一下各种情况：

- 中大奖的概率是两种情况之和：
    1. 选择射击小气球，中大奖， $\pi(a_1|s) p_{11}= 0.4 \times 0.25 = 0.1$
    2. 选择射击大气球，中大奖， $\pi(a_2|s) p_{21}= 0.6 \times 0.0 = 0.0$
    - 和为 $0.1+0.0=0.1$

- 脱靶的概率是两种情况之和：
    1. 选择射击小气球，脱靶， $\pi(a_1|s) p_{12}= 0.4 \times 0.7 = 0.28$
    2. 选择射击大气球，脱靶， $\pi(a_2|s) p_{22}= 0.6 \times 0.2 = 0.12$
    - 和为 $0.28+0.12=0.4$

- 中小奖的概率是两种情况之和：
    1. 选择射击大气球，中小奖， $\pi(a_2|s) p_{23}= 0.6 \times 0.8 = 0.48$
    2. 选择射击小气球，中小奖， $\pi(a_1|s) p_{13}= 0.4 \times 0.05 = 0.02$
    - 和为 $0.48+0.02=0.5$


所以这个结果和老板统计的第一轮的概率数值是一致的。

### 马尔可夫决策过程 MDP

MDP - Markov Decision Process

在前面的马尔科夫奖励过程中，我们学习过组成其过程的是一个四元组数据 $<S,P,R,\gamma>$，分别是状态、转移概率、奖励、折扣。进一步，在马尔可夫决策过程中，通过上一小节中引入的新元素“动作”，可以得到一个五元组数据 $ <S,A,P,R,\gamma>$，其中 $A$ 代表动作。

请读者注意上述五元组数据序列的顺序：
1. 必须从一个状态 $S$ 开始；
2. 执行一个动作 $A$；
3. 会以概率 $P$ 转移到下一个状态 $S'$；
4. 在转移过程中得到奖励 $R$；
5. 最后计算整个序列的奖励时有折扣 $\gamma$。

根据这个概念定义，绘制出图 5 的马尔可夫决策过程的模型。

<center>
<img src="./img/mdp-1.png">

图 5 马尔可夫决策过程模型
</center>

这是一个马尔可夫决策过程的**实例化**模型。为什么叫实例化模型呢？因为图中的各个子元素都带有序号。比如

- $\pi(a_1|s_1)$，表示在 $s_1$ 状态下，根据策略 $\pi$ 执行动作 $a_1$。其通用表达式为：

$$
\pi(a|s) = \mathbb P [A_t=a|S_t=s]
$$

- $p^{a_1}_{s_1s_2}$，表示在动作 $a_1$ 发生后，转移到状态 $s_2$ 的概率；
- $P^{a_1}_{ss'}$，表示 $a_1$ 动作下游的状态转移概率向量。其通用表达式为：

$$
P^a_{ss'}=\mathbb P [S_{t+1}=s'|S_t=s, A_t=a]
$$



<center>
<img src="./img/mdp-2.png">

图 5 马尔可夫决策过程模型
</center>

在图 5 中，简化了一些符号，比如状态转移概率用 $p_1,\cdots,p_5$ 表示，但是含义保持不变。但重点突出了红色的奖励机制。

三类

- 底层
    $r_1,\cdots,r_5$，分别表示从动作发生后，到达下个状态时的即时奖励值。如：射中小气球可以得到 3 分的奖励，脱靶 0 分，射中大气球得 1 分奖励。
    同一个动作分支的下游状态奖励值可以写成一个奖励向量（$n$ 为下游状态的数量）：

$$
R_s = [r_1, r_2, \cdots, r_n]
$$

- 中层
    $R_a(s)$ 表示执行动作 $a$ 的奖励函数，$a_1,a_2$ 表示两个不同动作，奖励函数分别是：
    - $R_{a_1}(s)=p_1 r_1 + p_2 r_2 + p_3 r_3$
    - $R_{a_2}(s)=p_4 r_4 + p_5 r_5$
    - 所以有通用写法（$n$ 为下游状态的数量）：

$$
\begin{aligned}
R_a(s) &= \mathbb E[R_{t+1}|S_t=s,A_t=a]
\\
&=\sum^n_{i=1} p_ir_i = P^a_{ss'}R_s
\end{aligned}
$$

- 顶层
    对于状态 $s_0$ 来说，由于不知道下一步将采取什么动作以及转移到哪一个状态，所以，只能把奖励函数定义为一个期望：
    $$
    \begin{aligned}
    R(s) &= \mathbb E[R_{t+1}|S_t=s]
    \\
    &=\pi_1 R_{a_1}(s) + \pi_2 R_{a_2}(s)
    \\
    &=\sum_{a \in A} \pi(a|s)R_a(s)
    \end{aligned}
    $$

    $A$ 是动作集合。