## 马尔可夫决策过程

### 射击气球问题

我们仍然以前面学过的、熟悉的状态转移问题来开始本章的学习。

有一天你去游乐场，有一个游戏的规则是这样的：

1. 游客花 5 元钱买 2 颗橡皮子弹
2. 墙上有两个气球，一个大的，一个小的
3. 击中大气球可以得到小奖，价值 1 元
4. 击中小气球可以得到小奖，价值 3 元

各个状态的转移概率如图 1 所示。

<center>
<img src="./img/shoot-1.png">

图 1
</center>

按游乐场老板的统计，第一发子弹：

- 游客中大奖的概率是 0.1
- 游客脱靶的概率是 0.3
- 游客中小奖的概率是 0.6

而第二发字典根据第一发子弹的结果而有所不同：

1. 如果第一发子弹中大奖
    - 第二发子弹继续中大奖的概率是 0.2，比初始的 0.1 提高了一倍，因为游客有经验了；
    - 第二发子弹脱靶的概率是 0.2，比初始的 0.3 要低；
    - 第二发子弹中小奖的概率不变，仍然是 0.6；
2. 如果第一发子弹脱靶
    第二发子弹的情况会和第一发子弹的初始概率一样，还是 0.1, 0.3, 0.6；
3. 如果第一发子弹中小奖
    - 第二发子弹中大奖的概率不变，仍然是 0.1；
    - 第二发子弹脱靶的概率是 0.2，比初始的 0.3 要低；
    - 第二发子弹继续中小奖的概率提高到 0.7。

那么一个聪明的游客该如何选择呢？





MDP - Markov Decision Process

前面章节中，曾经提到过两种奖励函数的方式：

- 面向结果，基于状态的奖励方法

    把过程分割成状态，把状态都看作是静止的瞬间。

- 面向过程，基于过程的奖励方法

    把看似静止的状态用过程来连接，形成时间轴上的前后关系。

任何事物都不是静止不动的，而是处于运动状态，只不过有的快有的慢。对于非智能物体，如空气流水等的运动，属于物理学范畴；而对于智能物体，如动物、人类等的行为，就属于强化学习的研究范畴了。



