## 马尔可夫决策过程

### 射击气球问题

我们仍然以前面学过的、熟悉的状态转移问题来开始本章的学习。

有一天你去游乐场，有一个游戏的规则是这样的：

1. 游客花 4 元钱买 2 颗橡皮子弹，得到两次射击机会；
2. 墙上有两个气球，一个大的，一个小的，游客可以任意选择目标；
3. 击中大气球可以得到小奖，价值 1 元；
4. 击中小气球可以得到大奖，价值 3 元。

按游乐场老板的统计，游客在第一次射击时：

- 中大奖的概率是 0.1
- 脱靶的概率是 0.3
- 中小奖的概率是 0.6

统计结果如图 1 所示。

<center>
<img src="./img/shoot-1.png">

图 1 游客中奖情况统计
</center>

而第二次射击根据第一次射击的结果而有所不同：

1. 如果第一次射击中大奖
    - 第二次射击继续中大奖的概率是 0.2，比初始的 0.1 提高了一倍，因为游客有经验了；
    - 第二次射击脱靶的概率是 0.2，比初始的 0.3 要低；
    - 第二次射击中小奖的概率不变，仍然是 0.6；
2. 如果第一次射击脱靶
    第二次射击的结果会和第一次射击的初始概率一样，还是 0.1, 0.3, 0.6；
3. 如果第一次射击中小奖
    - 第二次射击中大奖的概率不变，仍然是 0.1；
    - 第二次射击脱靶的概率是 0.2，比初始的 0.3 要低；
    - 第二次射击继续中小奖的概率提高到 0.7。

那么一个聪明的游客该如何选择呢？


利用前面学到过的知识，可以计算第一次射击时的三个状态，哪一个的状态价值函数值最大。于是有些读者前面的学习基础很牢靠，可以立刻画出如图 2 的状态转移图和转移概率表。

<center>
<img src="./img/shoot-2.png">

图 2 错误的状态转移图
</center>

其实这个问题的状态转移图应该如图 3 所示，是一个有向无环图。

<center>
<img src="./img/shoot-3.png">

图 3 正确的的状态转移图
</center>

说明

1. 虽然状态中有四个“大奖”，四个“小奖”，四个“脱靶”，但是这四个状态不是同一个状态，只是名字一样而已，在图 3 中特意用序号把它们都区分开了。其中：
    - 0 号为开始状态；
    - 1~3 号为二级状态；
    - 4~12 号为三级状态。
2. 开始状态需要有个 -4 的奖励，因为一开始游客是花 4 元钱买了两颗橡皮子弹。
3. 所有的三级状态（序号4~12），最后都接一个终止状态 T。

为什么四个“大奖”不是一个状态呢？

从“开始”状态看，第一次射击有 0.1 的概率到达 1 号“大奖”状态；而第二次射击有 0.2 的概率到达 4 号“大奖”状态。如果游客还有第三发子弹，那么中大奖的概率在第三轮还会提高。就好比从一楼上 10 级台阶到二楼，台阶可以定义为 1 到 10；而二楼到三楼同样有 10 级台阶，也定义为 1 到 10，但所处的楼层不一样。

我们从后向前计算一下各个状态的价值函数，由于是个有向无环的图，所以根据式 xxx 很简单就可以得到（本例中折扣应该为 1）：

首先，终止状态 $v_T = 0$。

然后 $v_4$ 为例：$v_4 = R_4 + \gamma 1.0 \times V_T = 3+1\times0=3$。其它三级状态的计算方法相同。

$$
\begin{cases}
v_T=0
\\
v_4=3, v_5 = 0, v_6 = 1, 
\\
v_7=3, v_8=0, v_9=1,
\\
v_{10}=3, v_{11}=0, v_{12}=1,
\\
v_1 = 3 + 0.2v_4+0.2v_5+0.6v_6=4.2
\\
v_2 = 0 + 0.1v_7+0.3v_8+0.6v_9=0.9
\\
v_3 = 1 + 0.1v_{10}+0.2v_{11}+0.7v_{12}=1.8
\\
v_0 = -4 + 0.1v_1+0.3v_2+0.6v_3=-2.23
\end{cases}
$$

所以，从统计学的观点看，只要游客买了子弹，就已经亏了，老板是稳赚不赔的。

那么游客应该怎么选择呢？

- 虽然 $V_{1}=4.2$ 状态价值最高，但是到达 $s_1$ 只有 0.1 的概率，可以理解为游客的收益是：$0.1\times4.2=0.42$元。
- $s_2$ 的收益是 $0.3\times0.9=0.27$ 元。
- $s_3$ 的收益是 $0.6\times1.8=1.08$ 元。

所以游客第一次射击时应该选择 $s_3$，即“小奖”状态，然后第二次射击时继续选择“小奖”状态。这样虽然肯定不能赢回开始花的 4 元钱，但是可以做到损失最小（或者收益最大）。


### 改进 引入动作/策略

仔细想一想，其实上面的解题过程是有问题的：

1. 原题是“聪明的游客会如何选择？”，但是整个解题思路是根据游乐场老板的统计结果进行的，是从老板的角度出发，而非游客的角度。
2. 在一级的三个状态中（大奖，小奖，脱靶），它只代表“结果”，而不代表“选择”，较真儿地说的话，没有游客会选择“脱靶”状态，因为“脱靶”只是个结果。
3. 忽略前面两个疑问，最后得到了 $v_1,v_2,v_3$ 的值，游客会看到 $v_1$ 最大，所以会误导其选择大奖而射击小气球，但是忽略了击中的难度。现实生活中人们往往也有类似的经历，想利益最大化，却忽略了风险。

实际上，老板并不知道游客选择的是哪个气球，他只看到了最终的结果。游客自己也不会主动说我要射击哪一个气球，只不过心里有数罢了。

经过对游客的调查与过程观察，我们得到了一些数据，如图 4 所示。

<center>
<img src="./img/shoot-4.png">

图 4 正确的的状态转移图
</center>


1. 在开始状态时，游客的策略（称之为 $\pi$）可以有两个：
    - $a_1$ - 选择射击小气球而中大奖，大概有 40% 的人选择。
    - $a_2$ - 选择射击大气球而中小奖，大概有 60% 的人选择。

2. 选择 $a_1$（记为 $\pi(a_1|s)$），在射击小气球时：
    - 只有 0.25 的概率打中，因为气球比较小，不容易击中，记为 $p_{11}=0.25$；
    - 却有 0.7 的概率脱靶，记为 $p_{12}=0.7$；
    - 还有 0.05 的概率很离谱地击中大气球而中小奖，记为 $p_{13}=0.05$。

3. 选择 $a_2$（记为 $\pi(a_2|s)$），在射击大气球时：
    - 有 0.8 的概率击中，因为气球比较大，容易击中，记为 $p_{23}=0.8$；
    - 有 0.2 的概率脱靶，记为 $p_{22}=0.2$；
    - 没有任何运气可以击中小气球，记为 $p_{21}=0.0$。

现在可以统计一下各种情况：

- 中大奖的概率是两种情况之和：
    1. 选择射击小气球，中大奖， $\pi(a_1|s) p_{11}= 0.4 \times 0.25 = 0.1$
    2. 选择射击大气球，中大奖， $\pi(a_2|s) p_{21}= 0.6 \times 0.0 = 0.0$
    - 和为 $0.1+0.0=0.1$

- 脱靶的概率是两种情况之和：
    1. 选择射击小气球，脱靶， $\pi(a_1|s) p_{12}= 0.4 \times 0.7 = 0.28$
    2. 选择射击大气球，脱靶， $\pi(a_2|s) p_{22}= 0.6 \times 0.2 = 0.12$
    - 和为 $0.28+0.12=0.4$

- 中小奖的概率是两种情况之和：
    1. 选择射击大气球，中小奖， $\pi(a_2|s) p_{23}= 0.6 \times 0.8 = 0.48$
    2. 选择射击小气球，中小奖， $\pi(a_1|s) p_{13}= 0.4 \times 0.05 = 0.02$
    - 和为 $0.48+0.02=0.5$




所以这就是老板统计的第一轮的结果。

MDP - Markov Decision Process

前面章节中，曾经提到过两种奖励函数的方式：

- 面向结果，基于状态的奖励方法

    把过程分割成状态，把状态都看作是静止的瞬间。

- 面向过程，基于过程的奖励方法

    把看似静止的状态用过程来连接，形成时间轴上的前后关系。

任何事物都不是静止不动的，而是处于运动状态，只不过有的快有的慢。对于非智能物体，如空气流水等的运动，属于物理学范畴；而对于智能物体，如动物、人类等的行为，就属于强化学习的研究范畴了。



