### 贝尔曼期望方程

有了模型和奖励，就可以进一步地研究价值问题了。如同马尔可夫奖励过程中的状态价值函数一样，在马尔可夫奖励过程过程中，同样会有价值函数，而且概念进一步地深化和扩展了。

在本节中，我们将会利用设计问题的具体案例，用倒推的方式来解释贝尔曼期望方程的具体含义，以方便读者理解。

#### 比较奖励过程和决策过程

先回忆一下在马尔可夫奖励过程中学习过的状态价值函数，温故而知新。

$$
\begin{aligned}
V(s) &= \mathbb E [G_t | S_t = s]
\\
&=\mathbb E [ R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \gamma^3 R_{t+4}+ \cdots]
\\
&=\sum_{s'} P_{ss'} R_{ss'}+ \gamma \sum_{s'} P_{ss'}V(s') 
\\
&=\sum_{s'} P_{ss'} [R_{ss'}+\gamma V(s')] 
\\
&= R(s)+ \gamma \sum_{s'} P_{ss'}V(s') 
\end{aligned}
\tag{1}
$$

<center>
<img src="./img/mdp-3.png">

图 1 左侧：马尔可夫奖励过程模型；右侧：马尔可夫决策过程模型。
</center>


比较图 1 中左侧和右侧虚线框内的部分，可以说除了符号不同，其它都是相同的，包括位置和含义：

||左侧|右侧|
|-|-|-|
|顶端|源状态 $s_0$，需要计算状态价值函数$v(s_0)$|源动作 $a_1$，需要计算动作价值函数$q_{\pi}(s_0,a_1)$|
|中间|状态转移概率$P_{ss'}$，过程奖励向量$R_{ss'}$|状态转移概率$P_{ss'}^{a_1}$，过程奖励向量$R_{ss'}^{a_1}$|
|底端|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v(s')$|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v_{\pi}(s')$|

所以，我们可以大胆地预测，计算动作价值函数 $q(s,a)$ 的公式与计算状态价值函数的贝尔曼方程完全一致。

$$
\begin{aligned}
Q(s,a) &= \mathbb E [G_t | S_t = s, A_t=a]
\\
&=\sum_{s'} p_{ss'}^a r_{ss'}^a+ \gamma \sum_{s'} p_{ss'}^a v(s') 
\\
&=\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v(s')] 
\\
&= R^a(s)+ \gamma P_{ss'}^a V(s') 
\end{aligned}
\tag{1}
$$


#### 处于第三层的价值函数

首先要注意，我们在图中是给过程定义的奖励，所以在从第二层的动作转移到第三层的状态过程中，是可以得到奖励的。但是第三层本身是没有任何奖励的，否则就会重复计算奖励。

由于 $s_1,\cdots,s_5$ 处于终点位置，可以看作是终止状态，所以按照定义，其状态价值函数为 0。

$v_i = 0, \quad (i=1,\cdots,5)$

#### 处于第二层的动作函数

由于在上下两层状态价值函数之间嵌入了一层动作节点，所以并不能直接得到第一层 $s_0$ 所处位置的状态函数，而是必须用第二层的的 $a_1,a_2$ 来衔接一下。所以，这里就必须引入对动作价值函数的定义。

$$
Q_\pi(s,a)=\mathbb E_\pi [G_t|S_t=s, A_t=a]
$$

读者可以忽略期望符号的下标 $\pi$，因为它也没有什么特殊的含义，就是表面这是具有策略参与的数学期望。期望公式内部的第一项 $G_t$，和价值函数中的 $G_t$ 定义相同，区别在后面的条件中，有 $S_t=s,A_t=a$ 并列两项，表示在状态 $s$ 下执行动作 $a$ 后获得的回报。对应到图 x 上，就是 $a_1$ 或 $a_2$ 后面的回报。

所以，动作价值函数在原理上和状态价值函数相同，都是对回报的期望。有两种获得办法：
- 如果模型未知，则使用采样求平均的方式，根据大数定律，可以近似得到动作价值 $q_\pi$。
- 如果模型已知，如本例中的转移概率是已知的，则可以用加权乘积和的方式直接获得数学期望。

具体到 $a_1$ 上，虽然 $v_1,v_2,v_3$ 都是 0，但是在状态转移过程中发生了 $r_1,r_2,r_3$，所以有：

$$
Q(s_0,a_1) = p_1 r_1 + p_2 r_2 + p_3 r_3 =\sum_{s'} p^{a_1}_{ss'} r^{a_1}_{ss'} = P^{a_1}_{ss'} R^{a_1}_{ss'}
$$

其中，$s=s_0, s' \in [s_1,s_2,s_3]$。

带入具体数据
$$
\begin{aligned}
Q(s_0,a_1) &= p_1 r_1 + p_2 r_2 + p_3 r_3
\\
&=0.2 \times 3 + 0.75 \times 0 + 0.05 \times 1
\\
&=0.65
\end{aligned}
$$

$$
\begin{aligned}
Q(s_0,a_2) &= p_4 r_4 + p_5 r_5 
\\
&=0.4 \times 0 + 0.6 \times 1
\\
&=0.6
\end{aligned}
$$


#### 处于第一层的状态价值函数




$$
V_\pi(s)=\mathbb E_\pi [G_t|S_t=s]
$$

与动作价值函数一样，有模型时可以直接用加权乘积和来获得

$$
\begin{aligned}
V_\pi(s_0)&=\pi_1 Q(s_0,a_1) +  \pi_2 Q(s_0,a_2)
\\
&=0.4 \times 0.65 + 0.6 \times 0.6
\\
&=0.62
\end{aligned}
$$



#### 动作价值函数

