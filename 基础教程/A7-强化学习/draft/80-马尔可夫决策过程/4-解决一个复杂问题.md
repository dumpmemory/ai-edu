
因为在射击问题中，有以下几个特点：

- 是一个单向选择和转移的过程，最后形成了一张有向无环图。这就决定了我们可以从后向前一步步**回溯**，自下向上逐层计算状态价值函数 $v_\pi(s)$ 和动作价值函数 $q_\pi(s,a)$，直到开始状态。其计算过程是：$v_T \to q_\pi \to v_\pi \to q_\pi \to v_\pi$，如图 mdp-8 所示。

- 每个分支的细节分析起来比较多，但实际上分支之间的区别只存在于状态转移的过程的具体概率数值上。比如图 mdp-7 所示，其中的“红3”动作后的状态转移概率是 [0.25,0.7.0.05]，而动作“红5”的状态转移概率是 [0.2,0.75,0.05]。这种细节只是导致计算结果不同，但概念是相同的。

- 而另外一个重要的策略选择问题，我们暂时用“红:蓝=0.4:0.6”的统计结果设定了，在两次射击时都是如此。但是在实际问题中，游客各有各的策略，这种统一的设定只是一种统计结果。游乐场老板虽然精明，但是游客们也不是傻子，谁都知道连续两次选择红色气球是有机会得到最高值 6 分的奖励的，只不过是能不能顺利实现的问题。




### 二级回溯

从前面的推导中，可以在已知 $q_\pi(s,a)$ 时计算出 $v_\pi(s)$（式5），也可以已知 $v_\pi(s)$ 计算出 $q_\pi(s,a)$（式2），这似乎变成了一个鸡生蛋蛋生鸡的死循环问题。能不能像贝尔曼方程那样，从$v_\pi(s')$ 计算出 $v_\pi(s)$ 来，从$q_\pi(s',a')$ 计算出 $q_\pi(s,a)$ 来呢？这样的话，我们就可以继续利用前面学过的迭代法或者矩阵法来方便地解出状态价值函数和动作价值函数了。

先绘制出两张二级回溯图，方便公式推导。如图 6 所示。

<center>
<img src="./img/mdp-7.png">

图 6
</center>

先看左图，我们的目的是想从 $v_\pi(s')$ 得到 $v_\pi(s)$，中间隔着一个 $q_\pi(s,a)$。如果把 $q_\pi(s,a)$ 的表达式带入 $v_\pi(s)$ 的表达式，就可以达到消去 $q_\pi(s,a)$ 的目的了。


于是可以把式 2.1 的 $q_\pi(s,a)$ 带入式 5：


$$
\begin{aligned}
v_\pi(s) &= \sum_a \pi(a|s)q_\pi(s,a) &(式5)
\\
(带入式2.1替换 q_\pi \to)&=\sum_a \pi(a|s) \Big(\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')]\Big) &(10.1)
\\
(矩阵形式\to)&=\sum_a \pi(a|s) \Big(P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')]\Big) &(10.2)
\\
(动作奖励函数形式\to)&=\sum_a \pi(a|s) \Big( R^a(s)+ \gamma P_{ss'}^a V_\pi(s') \Big) &(10.3)
\end{aligned}
\tag{10}
$$

式 10 就是 $v_\pi$ 的迭代表达式。

再看右图，想把$v_\pi(s')$ 消掉，需要先得到它的表达式。

从式 5 把 $s,a$ 换成 $s',a'$，可以得到式 11：

$$
v_\pi(s') = \sum_{a'} \pi(a'|s')q_\pi(s',a')
\tag{11}
$$

把式 11 带入式 2：

$$
\begin{aligned}
q_\pi(s,a)&=\sum_{s'} p_{ss'}^a \big [r_{ss'}^a+\gamma v_\pi(s') \big ] &(式2.1)
\\
(带入式11替换v_\pi\to)&=\sum_{s'} p_{ss'}^a \Big ( r_{ss'}^a+\gamma \sum_{a'} [\pi(a'|s')q_\pi(s',a') ] \Big) &(12.1)
\\
(动作奖励函数形式 \to)&=R^a(s)+\gamma \sum_{s'} p_{ss'}^a \Big ( \sum_{a'} [\pi(a'|s')q_\pi(s',a')] \Big) &(12.2)
\end{aligned}
\tag{12}
$$

式 12 就是 $q_\pi$ 的迭代的表达形式。

本节的公式有些多，下面总结一下，便于读者以后速查。

表 3 $v_\pi, q_\pi$ 的互换速查表

|=|$v_\pi$|$q_\pi$|
|-|-|-|
|$v_\pi$|$v_\pi(s)=\sum_a \pi(a \mid s) \Big(\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')]\Big)$<br>(式10.1)|$v_\pi(s) = \sum_{a} \pi(a \mid s)q_\pi(s,a)$ <br>(式5)|
|$q_\pi$|$q_\pi(s,a)=\sum_{s'} p_{ss'}^a \big [r_{ss'}^a+\gamma v_\pi(s') \big ]$<br> (式2.1)|$q_\pi(s,a)=\sum_{s'} p_{ss'}^a \Big ( r_{ss'}^a+\gamma \sum_{a'} [\pi(a'\mid s')q_\pi(s',a') ] \Big)$<br> (式12.1)|

在表 3 中，我们只给出了公式的原始形式，读者可以在实际用使用矩阵形式或是奖励函数形式。


