## 马尔可夫决策过程

### 马尔可夫决策过程 MDP

MDP - Markov Decision Process

在上一节中，我们在上下游的状态之间引入了策略和动作，即，在马尔可夫奖励过程中引入策略和动作的概念，就形成了一个新的概念：马尔可夫决策过程。

在前面的**马尔科夫奖励**过程中，我们学习过组成其过程的是一个四元组数据 $<S,P,R,\gamma>$，分别是状态、转移概率、奖励、折扣。进一步，在**马尔可夫决策**过程中，通过上一小节中引入的新元素“动作”，可以得到一个五元组数据 $ <S,A,P,R,\gamma>$，其中 $A$ 代表动作。

请读者注意上述五元组数据序列的顺序：
1. 必须从一个状态 $S$ 开始；
2. 执行一个动作 $A$；
3. 会以概率 $P$ 转移到下一个状态 $S'$；
4. 在转移过程中得到奖励 $R$；
5. 最后计算整个序列的回报时可以有折扣 $\gamma \in [0,1]$。

表 1 三种过程的简单比较

|过程名称|组成元素|数据序列|
|-|-|-|
|马尔可夫过程 MP|$<S,P>$|$S_0,S_1,\cdots,S_t$|
|马尔可夫奖励过程 MRP|$<S,P,R,\gamma>$|$S_0,R_1,S_1,R_2,\cdots,S_t,R_{t+1}$|
|马尔可夫决策过程 MDP|$<S,A,P,R,\gamma>$|$S_0,A_0,R_1,S_1,A_1,R_2,\cdots,S_t,A_t,R_{t+1}$|

#### 动作模型

<center>
<img src="./img/mdp-state-action.png">

图 5 马尔可夫决策过程模型
</center>


#### 状态转移模型

<center>
<img src="./img/mdp-action-state.png">

图 5 马尔可夫决策过程模型
</center>

#### 全模型

根据这个概念定义，绘制出图 5 的马尔可夫决策过程的模型。

<center>
<img src="./img/mdp-1.png">

图 5 马尔可夫决策过程模型
</center>

图 5 实际上是图 4 的一个抽象，去掉了与应用场景相关的表达，但又不是抽象到无法理解，所以这是一个马尔可夫决策过程的**实例化**模型。为什么叫实例化模型呢？因为图中的各个子元素都带有序号，而不是笼统抽象的符号。比如：

- 状态

    用空心的圆表示，里面标出了状态序号，如 $s_0, s_1$ 等。

    两侧的 $s,s'$ 和 $S_t,S_{t+1}$，是一种概念的两种表达形式，在公式推导中，一般会用 $s,s'$ 表示当前状态和下一个状态的**实例**，用 $S_t,S_{t+1}$ 来强调**时序**关系的和状态**变量**。

- 动作

    用实心的圆表示，里面标出了动作序号，如 $a_1,a_2$ 等。
    动作一定是源于状态、终于状态，不可能两个动作直接连接在一起。动作是策略选择的结果。

- 策略

    用 $\pi(a|s)$ 表示在状态 $s$ 下选择动作 $a$。蓝色的弧形虚线表示作用范围。
    特别地，$\pi(a_1|s_0)$，表示在 $s_0$ 状态下，根据策略 $\pi$ 执行动作 $a_1$。其通用定义为：

    $$
    \pi(a|s) = \mathbb P [A_t=a|S_t=s] \tag{1}
    $$

    用文字描述为：在时刻 $t$ 的状态为 $s$ 时选择动作 $a$ 的概率。大写的 $A_t,S_t$ 表示变量或集合，小写的 $a,s$ 表示实例数值。

- 转移概率
    每个动作下面都会存在一个转移概率，如果只有一个下游状态存在，则转移概率值为 1。绿色的弧形虚线表示作用范围。

    - $p^{a_1}_{s_0s_1}$，小写的 $p$ 表示在动作 $a_1$ 发生后，从状态 $s_0$ 转移到状态 $s_1$ 的概率的实例；
    - $P^{a_1}_{ss'}$，大写的 $P$ 表示 $a_1$ 动作的下游状态转移概率向量，在本例中包括三个元素 $P^{a_1}_{ss'}=[ p^{a_1}_{s_0 s_1},p^{a_1}_{s_0 s_2},p^{a_1}_{s_0 s_3}]$。其通用表达式为：

    $$
    P^a_{ss'}=\mathbb P [S_{t+1}=s'|S_t=s, A_t=a] \tag{2}
    $$

    用文字描述为：在时刻 $t$ 的状态 $s$ 下采取动作 $a$ 后转移到时刻 $t+1$ 的状态 $s'$ 的概率。大写的符号是变量，小写的符号是实例。

- 过程

    图 5 中带有箭头的线都表示过程，有两类：
    - 蓝色实线：表示策略 $\pi$ 的动作选择过程。**这是强化学习的主要学习目标**。
    - 绿色虚线：表示概率 $P$ 的状态转移过程。这里用绿色实线也可以的，但是用虚线可以给色弱的朋友们提供更好的帮助，以便可以轻松地与实线区分开来。

#### 奖励模型

在奖励模型中，我们使用了面向过程的奖励方法。与在**马尔可夫奖励过程**中学习过的面向过程的奖励方法相似，区别是：

- 在前面的**马尔可夫奖励过程**中，定义的是 $R_{ss'}$，即状态到状态的转移过程中产生的奖励。
- 在本节的**马尔可夫决策过程**中，定义比较复杂，请看图 5。

<center>
<img src="./img/mdp-reward.png">

图 5 马尔可夫决策过程模型
</center>

在图 5 中，简化了一些符号，比如状态转移概率用 $p_1,\cdots,p_5$ 表示（图 4 中用 $p^a_{ss'}$ 表示），含义与图 4 一致。但重点突出了红色的奖励机制。

在图 5 中，共有三层元素来描述整体的奖励模型：

- 底层
    $r_1,\cdots,r_5$，分别表示从动作发生后，到达下个状态时的即时奖励值。如：射中小气球可以得到 3 分的奖励，脱靶 0 分，射中大气球得 1 分奖励。
    同一个动作分支的下游过程奖励值可以写成一个奖励向量（$n$ 为下游状态的数量）：

    $$
    R^a_{ss'} = [r_1, r_2, \cdots, r_n] \tag{3}
    $$

    其中的 $r_1,\cdots,r_n$ 也可以抽象地写成 $r^a_{ss'}$，便于后面用公式抽象表达。

- 中层
    $R^a(s)$ 表示执行动作 $a$ 的奖励函数，简称为**动作奖励函数**，在有的文献中记为 $R^a_s$，其含义一致。笔者认为，如果定义为**函数**，就应该有个形式化的括号来表示。
    这个奖励函数不是凭空产生的，是根据底层的过程奖励计算得到的，它发生在动作节点上。如果在其它学习资料的案例中发现有这种写法，请读者一定要明白其来源。
    
    $a_1,a_2$ 表示两个不同动作，奖励函数分别是：
    - $R^{a_1}(s)=p_1 r_1 + p_2 r_2 + p_3 r_3$
    - $R^{a_2}(s)=p_4 r_4 + p_5 r_5$
    
    有通用写法（$n$ 为下游状态的数量）：

    $$
    \begin{aligned}
    R^a(s) &= \mathbb E[R_{t+1}|S_t=s,A_t=a]
    \\
    &=\sum^n_{i=1} p_ir_i = \sum_{s'} p^a_{ss'} r^a_{ss'} = P^a_{ss'}R^a_{ss'}
    \end{aligned}
    \tag{4}
    $$

    以 $a_1$ 为例，在实际的状态转移过程中，只可能有 $r_1,r_2,r_3$ 三者中的之一发生，要看运气（概率），即游客要么中大奖，要么中小奖，要么不中奖，没有其它的组合。但是在模型描述时，只能用这种数学期望来表示。


- 顶层
    对于状态 $s_0$ 来说，由于不知道下一步将采取什么动作以及转移到哪一个状态，所以，只能把奖励函数定义为一个期望：
    $$
    \begin{aligned}
    R(s) &= \mathbb E[R_{t+1}|S_t=s]
    \\
    &=\pi_1 R^{a_1}(s) + \pi_2 R^{a_2}(s)
    \\
    &=\sum_{a \in A(s)} \pi(a|s)R^a(s)
    \end{aligned}
    \tag{5}
    $$

    其中 $A$ 是动作集合（或称为动作空间），在本例中只有**射击红色气球**和**射击蓝色气球**两种动作选择。

    注意，这个奖励函数也不是凭空产生的，而是在 $R^a(s)$ 的基础上的定义，相当于是面向状态的奖励。

**在实际的计算中，只能使用这个奖励模型中的三层的任一层，它们是互斥的，否则就会造成重复计算奖励。**
