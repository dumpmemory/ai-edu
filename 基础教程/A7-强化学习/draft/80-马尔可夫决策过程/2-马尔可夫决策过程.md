## 马尔可夫决策过程

### 马尔可夫决策过程 MDP

MDP - Markov Decision Process

在马尔可夫奖励过程中引入动作的概念，就形成了一个新的概念：马尔可夫决策过程。

在前面的马尔科夫**奖励**过程中，我们学习过组成其过程的是一个四元组数据 $<S,P,R,\gamma>$，分别是状态、转移概率、奖励、折扣。进一步，在马尔可夫**决策**过程中，通过上一小节中引入的新元素“动作”，可以得到一个五元组数据 $ <S,A,P,R,\gamma>$，其中 $A$ 代表动作。

请读者注意上述五元组数据序列的顺序：
1. 必须从一个状态 $S$ 开始；
2. 执行一个动作 $A$；
3. 会以概率 $P$ 转移到下一个状态 $S'$；
4. 在转移过程中得到奖励 $R$；
5. 最后计算整个序列的奖励时有折扣 $\gamma$。

#### 状态/动作模型

根据这个概念定义，绘制出图 5 的马尔可夫决策过程的模型。

<center>
<img src="./img/mdp-1.png">

图 5 马尔可夫决策过程模型
</center>

图 5 实际上是图 4 的一个抽象，去掉了与应用场景相关的表达，但又不是抽象到无法理解，所以这是一个马尔可夫决策过程的**实例化**模型。为什么叫实例化模型呢？因为图中的各个子元素都带有序号。比如：

- 状态

    用空心的圆表示，里面标出了状态序号，如 $s_0, s_1$ 等。

    两侧的 $S,S'$ 和 $S_t,S_{t+1}$，是一种概念的两种表达形式，在公式推导中，有时候会用 $S,S'$ 表示当前状态和下一个状态的实例，有时候会用 $S_t,S_{t+1}$ 来强调时序关系的和状态变量。

- 动作

    用实心的圆表示，里面标出了动作序号，如 $a_1,a_2$ 等。
    动作一定是源于状态、终于状态，不可能两个动作直接连接在一起。

- 策略

    用 $\pi(a|s)$ 表示在状态 $s$ 下选择动作 $a$。蓝色的弧形虚线表示作用范围。
    特别地，$\pi(a_1|s_1)$，表示在 $s_1$ 状态下，根据策略 $\pi$ 执行动作 $a_1$。其通用表达式为：

    $$
    \pi(a|s) = \mathbb P [A_t=a|S_t=s] \tag{1}
    $$

    用文字描述为：在时刻 $t$ 的状态为 $s$ 时，选择动作 $a$ 的概率，大写的 $A_t,S_t$ 表示变量，小写的 $a,s$ 表示实例数值。

- 转移概率
    每个动作下面都会存在一个转移概率，如果只有一个下游状态存在，则转移概率值为 1。绿色的弧形虚线表示作用范围。

    - $p^{a_1}_{s_0s_1}$，表示在动作 $a_1$ 发生后，从状态 $s_0$ 转移到状态 $s_1$ 的概率；
    - $P^{a_1}_{ss'}$，表示 $a_1$ 动作下游的状态转移概率向量，在本例中包括三个元素。其通用表达式为：

    $$
    P^a_{ss'}=\mathbb P [S_{t+1}=s'|S_t=s, A_t=a] \tag{2}
    $$

    用文字描述为：在时刻 $t$ 的状态 $s$ 下，采取动作 $a$ 后，转移到时刻 $t+1$ 的状态 $s'$ 的概率。大写的符号是变量，小写的符号是实例。

- 过程

    图 5 中带有箭头的线都表示过程，有两类：
    - 蓝色实线：表示策略 $\pi$ 的动作选择过程。
    - 绿色虚线：表示概率 $P$ 的状态转移过程。这里用绿色实线也可以的，但是用虚线可以给色弱的朋友们提供更好的帮助，以便可以轻松地与实线区分开来。

#### 奖励模型

在奖励模型中，我们使用了面向过程的奖励方法。与在**马尔可夫奖励过程**中学习过的面向过程的奖励方法相似，区别是：

- 在前面的**马尔可夫奖励过程**中，定义的是 $R_{ss'}$，即状态到状态的转移过程中产生的奖励。
- 在本节的**马尔可夫决策过程**中，定义比较复杂，请看图 5。

<center>
<img src="./img/mdp-2.png">

图 5 马尔可夫决策过程模型
</center>

在图 5 中，简化了一些符号，比如状态转移概率用 $p_1,\cdots,p_5$ 表示，含义与图 4 一致。但重点突出了红色的奖励机制。

在图 5 中，共有三层元素来描述整体的奖励模型：

- 底层
    $r_1,\cdots,r_5$，分别表示从动作发生后，到达下个状态时的即时奖励值。如：射中小气球可以得到 3 分的奖励，脱靶 0 分，射中大气球得 1 分奖励。
    同一个动作分支的下游状态奖励值可以写成一个奖励向量（$n$ 为下游状态的数量）：

$$
R_s = [r_1, r_2, \cdots, r_n] \tag{3}
$$

- 中层
    $R_a(s)$ 表示执行动作 $a$ 的奖励函数，在有的文献中记为 $R^a_s$，其含义一致。笔者认为，如果定义为奖励**函数**，就应该有个形式化的括号来表示。
    
    $a_1,a_2$ 表示两个不同动作，奖励函数分别是：
    - $R_{a_1}(s)=p_1 r_1 + p_2 r_2 + p_3 r_3$
    - $R_{a_2}(s)=p_4 r_4 + p_5 r_5$
    - 所以有通用写法（$n$ 为下游状态的数量）：

    $$
    \begin{aligned}
    R_a(s) &= \mathbb E[R_{t+1}|S_t=s,A_t=a]
    \\
    &=\sum^n_{i=1} p_ir_i = P^a_{ss'}R_s
    \end{aligned}
    \tag{4}
    $$

    以 $a_1$ 为例，在实际的状态转移过程中，只可能有 $r_1,r_2,r_3$ 三者中的之一发生，要看运气（概率），即游客要么中大奖，要么中小奖，要么不中奖，没有其它的组合。但是在数学模型描述上，只能用这种数学期望来表示。



- 顶层
    对于状态 $s_0$ 来说，由于不知道下一步将采取什么动作以及转移到哪一个状态，所以，只能把奖励函数定义为一个期望：
    $$
    \begin{aligned}
    R(s) &= \mathbb E[R_{t+1}|S_t=s]
    \\
    &=\pi_1 R_{a_1}(s) + \pi_2 R_{a_2}(s)
    \\
    &=\sum_{a \in A} \pi(a|s)R_a(s)
    \end{aligned}
    \tag{5}
    $$

    其中 $A$ 是动作集合（或称为动作空间），在本例中只有**射击红色气球**和**射击蓝色气球**两种动作选择。
